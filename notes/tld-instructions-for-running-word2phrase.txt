Download the two data sources from PubMed described here:
http://bio.nlplab.org/#source-data

Use the nxml2txt script  on github  to preprocess the data:
https://github.com/spyysalo/nxml2txt

Download and compile - the provided make file should suffice -
word2vec from here: https://code.google.com/p/word2vec/

Run the following in order to produce the vanilla and phrase vector models:

# vanilla :
time ./word2vec -train NEURO -output NEURO-VECTOR -cbow 0 -size 300
-window 10 min-count 5 -negative 0 -hs 1 -sample 1e-3 -threads 12
-binary 1
                                       # phrase :
time ./word2phrase -train NEURO -output NEURO-PHRASE -threshold 500 -debug 2
# should take less that an hour on a multi-core machine with 16GB or more
time ./word2vec -train NEURO-PHRASE -output NEURO-PHRASE-VECTOR -cbow
0 -size 300 -window 10 min-count 5 -negative 0 -hs 1 -sample 1e-3
-threads 12 -binary 1
# should take around 2 hours on a multi-core machine with 16GB or more

# notes:
# 1. Doesn't really matter regarding SkipGram versus CBoW. SkipGram is fine.
# 2. Preprocessing can help. If you don't care about punctuation or
case you can strip them.  As for stop words, the -sample flag in
word2vec will downsample frequent terms, so you don't need to
explicitly remove stop words. Doesn't hurt of course if you really
want to remove them. If you think bigrams and trigram are really
important, you can use word2phrase to chunk them.
# 3. Doesn't really matter. HS is fine.
# 4. Notes on parameters:
# size - 200 is fine. Reasonable range is 100-1000, 500 is often the best.
# alpha - 0.025, I think this is the default in the source, it's a good value.
# window - 5 is fine, I've also used 10 and 20 on wikipedia.
# min_count - 5 is fine, I've also used 10.
# threads - 1 is slow. Use 10 or 12.
# if you want to start a build, log off and return later to use your
model, then use bash and nohup, e.g., build a 500-D embedding space:

# nohup time ./word2vec -train NEURO -output NEURO-VECTOR-500 -cbow 0
-size 500 -window 10 min-count 10 -negative 0 -hs 1 -sample 1e-3
-threads 12 -binary 1 > NOHUP.out 2> NOHUP.err < /dev/ &

# nohup time ./word2vec -train NEURO-PHRASE -output
NEURO-PHRASE-VECTOR-500 -cbow 0 -size 500 -window 10 min-count 10
-negative 0 -hs 1 -sample 1e-3 -threads 12 -binary 1 > NOHUP.out 2>
NOHUP.err < /dev/ &

# Use Gensim to load and experiment with vectors, e.g.,

# neuro_lang_model = gensim.models.Word2Vec.load_word2vec_format(
neuro_code_base + 'models/NEURO-VECTOR', binary=True )

------------------------------------------

Notes from Adam's first attempt:

On Ubuntu, to compile word2vec, using the 8.2 GB of PMC shards arising from Tom's script:

cd ~/Desktop
sudo apt-get install subversion
svn checkout http://word2vec.googlecode.com/svn/trunk/
cd trunk
make

cd ../PubMed/shards/
cat * > ../concatenated-PMC-shards.txt
cd ../../trunk

time ./word2vec -train ~/Desktop/PubMed/concatenated-PMC-shards.txt -output ~/Desktop/NEURO-VECTOR -cbow 0 -size 300 -window 10 min-count 5 -negative 0 -hs 1 -sample 1e-3 -threads 12 -binary 1
time ./word2phrase -train ~/Desktop/PubMed/concatenated-PMC-shards.txt -output ~/Desktop/NEURO-PHRASE -threshold 500 -debug 2
time ./word2vec -train ~/Desktop/NEURO-PHRASE -output ~/Desktop/NEURO-PHRASE-VECTOR -cbow 0 -size 300 -window 10 min-count 5 -negative 0 -hs 1 -sample 1e-3 -threads 12 -binary 1

which gave the results

r@Puget-126251:~/Desktop/trunk$ time ./word2vec -train ~/Desktop/PubMed/concatenated-PMC-shards.txt -output ~/Desktop/NEURO-VECTOR -cbow 0 -size 300 -window 10 min-count 5 -negative 0 -hs 1 -sample 1e-3 -threads 12 -binary 1
Starting training using file /home/user/Desktop/PubMed/concatenated-PMC-shards.txt
Vocab size: 1884503
Words in train file: 1283094994
Alpha: 0.000002  Progress: 99.99%  Words/thread/sec: 11.75k  
real	247m29.512s
user	1823m42.896s
sys	4m28.745s
user@Puget-126251:~/Desktop/trunk$ time ./word2phrase -train ~/Desktop/PubMed/concatenated-PMC-shards.txt -output ~/Desktop/NEURO-PHRASE -threshold 500 -debug 2
Starting training using file /home/user/Desktop/PubMed/concatenated-PMC-shards.txt
Words processed: 1244500K     Vocab size: 136672K  
Vocab size (unigrams + bigrams): 76236629
Words in train file: 1244511348
Words written: 1244500K
real	47m0.507s
user	45m38.300s
sys	1m18.670s
user@Puget-126251:~/Desktop/trunk$ time ./word2vec -train ~/Desktop/NEURO-PHRASE -output ~/Desktop/NEURO-PHRASE-VECTOR -cbow 0 -size 300 -window 10 min-count 5 -negative 0 -hs 1 -sample 1e-3 -threads 12 -binary 1
Starting training using file /home/user/Desktop/NEURO-PHRASE
Vocab size: 2686389
Words in train file: 1227632953
Alpha: 0.000002  Progress: 100.00%  Words/thread/sec: 12.80k  
real	219m11.572s
user	1603m49.795s
sys	4m5.375s

